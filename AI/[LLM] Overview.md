[Velog로 가기](https://velog.io/@choi-hyk/LLM-Overview)

released at 2025-08-16 16:48:18 KST

updated at 2025-11-18 17:12:01 KST

|[AI](https://velog.io/tags/AI)|[LLM](https://velog.io/tags/LLM)|
|----|----|

# 📖 Overview...

이전 글에 **Prompt Engineering**과 **Chunking**에 대해서 정리를 했었는데, **LLM의 기초**부터 Velog에 정리를 해야 할 필요를 느꼈다. **LLM의 개념**에 대해서는 예전에 책으로 몇 번 보고, 영상이나 강의자료로 가볍게 본 기억이 있는데, 이번에 제대로 기초부터 다시 공부해서 정리해 보려고 한다.

이번 글에서는 간단한 **LLM의 역사**와 **기본적인 원리**를 간단하게 정리해 보고, 다음 글에서 **Transformer**에 대해서 심도 있게 다룰 생각이다.

---

## 📜 LLM History

**LLM(Large Language Model)** 은 처음 들어보면, 엄청나게 복잡한 알고리즘과 원리로 동작하는 것처럼 보인다. 하지만 **기본적인 원리**는 엄청 간단하다고 한다. **뒷말 잇기**를 생각해 보면 이해가 될 텐데, 만약 이러한 문장을 보았다고 하자.

`나는 늦게 일어나서 학교까지 ~`

나는 뒤에 **`뛰어갔다`** 를 넣으면 자연스러울 것 같다. 아마도 **`택시를 타고 갔다`** 도 괜찮을 것 같다. 그럼 **LLM**이 볼 때는 어떻게 생각을 할까? LLM은 수많은 예시를 가지고 있고, 각 예시는 **확률**을 가지고 있다.

| 후보 단어    | 확률(%)   |
| -------- | ------- |
| **뛰어갔다** | **35%** |
| **택시를**  | **25%** |
| **걸어갔다** | **15%** |
| **버스를**  | **10%** |
| **지각했다** | **5%**  |
| **기타**   | **10%** |

만약 이러한 확률을 가지고 있다고 해보자. **LLM**은 아마도 이러한 확률 테이블에서 가장 적절한 후보 단어를 골라서 문장을 생성해 낼 것이다.

이것이 바로 **기본적인 LLM의 동작**이다. 이러한 원리는 **1950년대**부터 고안이 되었는데, 그 유명한 **튜링 머신 테스트**가 이러한 **자연어 생성 메커니즘**에 부합하는 기계를 찾는 테스트이다.

이후 **1990년대**에는 **통계적 언어 모델**이 등장했다. **N-gram**이라는 모델을 사용해 이전 단어들을 보고 다음 단어의 확률을 계산하는 방식이었다. 하지만 긴 문맥을 처리하지 못하고 데이터가 커질수록 **희소성 문제**가 발생했다.

**2010년대 초반**, **RNN(Recurrent Neural Network)** 과 **LSTM(Long Short-Term Memory)** 같은 신경망 모델이 **NLP**에 도입되면서 조금 더 긴 문맥을 다룰 수 있게 되었지만, 여전히 **학습 속도**와 **긴 시퀀스 처리**에서 한계가 있었다.

> 참고로 **RNN**은 **순환 신경망** 기술로 연쇄적인 데이터를 처리하기 위해 **이전 상태**를 입력으로 받아서 출력을 만들어 내는 **뉴런 구조**에서 착안한 기술이다.

결정적인 전환점은 **2017년 Transformer**의 등장이다. **「Attention is All You Need」** 라는 논문에서 제안된 Transformer 구조는 **병렬 처리**가 가능하면서도 **긴 문맥**을 효과적으로 학습할 수 있게 했다. 그 이후 **BERT, GPT, T5** 같은 모델들이 등장하며 **언어 모델의 패러다임**을 완전히 바꾸었다.

---

## 🤖 BERT / GPT / T5

**Transformer**는 입력된 시퀀스의 모든 단어를 **병렬로 처리**할 수 있다. 이러한 병렬 처리를 가능하게 하는 것이 바로 **멀티헤드 어텐션(MultiHead Attention)**이다. **멀티헤드 어텐션**은 단어들의 **관계**를 파악하는 기법이다.

**Transformer**는 크게 두 가지의 구조로 구성된다. **인코더(Encoder)**와 **디코더(Decoder)** 두 개의 절차로 구성되는데, 이를 어떻게 사용하느냐에 따라 **BERT, GPT, T5**와 같은 다양한 모델이 탄생했다.

### BERT (Bidirectional Encoder Representations from Transformers)

##### Bidirectional → 양방향

**BERT**는 **구글**에서 개발한 모델로, **인코더만**으로 구성되어 있다. **양방향(Bidirectional)** 으로 문맥을 학습하는 것이 특징이다. 예를 들어, `나는 늦게 일어나서 학교까지 뛰어갔다`라는 문장이 있을 때, **BERT**는 "나는 늦게 일어나서"와 "학교까지"라는 **양쪽의 문맥**을 모두 고려하여 **`뛰어갔다`**라는 단어를 이해한다. 이는 문장의 **의미를 이해하고 분류하는 과제(NLU, Natural Language Understanding)** 에 매우 효과적이다.

### GPT (Generative Pre-trained Transformer)

**GPT**는 **OpenAI**에서 개발한 모델로, **디코더만**으로 구성되어 있다. **GPT**는 **단방향(Unidirectional)** 으로 학습하며, 문맥을 기반으로 **다음 단어를 예측**하는 방식이다. 위 예시에서, GPT는 `나는 늦게 일어나서 학교까지`라는 문장이 주어졌을 때, **이전 단어들**만을 참고하여 \*\*`뛰어갔다`\*\*를 예측한다. 이 구조는 새로운 문장을 **생성하는 과제(NLG, Natural Language Generation)** 에 뛰어나다.

### T5 (Text-to-Text Transfer Transformer)

**T5**는 **구글**에서 개발한 모델로, **인코더와 디코더**를 모두 사용한다. **T5**의 가장 큰 특징은 모든 자연어 처리 문제를 **"텍스트를 텍스트로 바꾸는(text-to-text)"** 형식으로 통일했다는 점이다. 예를 들어, **문장 분류, 요약, 번역** 등 모든 과제를 **질문과 답변 텍스트 쌍**으로 변환하여 학습한다.

자세한 원리는 다음 글인 **`Transformer`**에서 심도 있게 다뤄보도록 하겠다.

---

## 🌟 Emergence

그런데 우리가 궁금한 것은 이러한 소위 말하는 **LLM 혁명**이 어떻게 왔냐는 것이다. 아마 **LLM**에 관심이 많으면 **창발적 능력**이라는 말을 많이 들어봤을 텐데, 답은 여기에 있다.

**창발적 능력**이란 **작은 모델**에서는 전혀 보이지 않던 능력이, **모델의 규모**가 일정 임계치를 넘었을 때 **갑작스럽게 도약**하듯 나타나는 현상을 의미한다.

<img width="1129" height="833" alt="Image" src="https://github.com/user-attachments/assets/0f419745-167a-4a72-ad47-a56d8ed5341b" />  

해당 그래프는 **OpenAI**가 **2017년**에 발표한 논문 **「Learning to Generate Reviews and Discovering Sentiment Neurons」** 에서 나온 결과인데, **LSTM(Long Short-Term Memory)** 기반의 언어 모델이 갑자기 **긍정/부정 감정(sentiment)**을 구분하는 능력을 갖추게 된 것이다. 이것이 초기 **창발적 능력**의 증거라고 보는 견해가 많다.

이러한 **창발적 능력**은 **GPT-2**에서도 관찰되었다는 견해가 있지만, 실제로 놀라운 성능을 보여준 것은 **GPT-3**부터였다. 바로 **`In Context Learning`**이라는 **Prompt Engineering**의 핵심이 되는 현상이 일어난 것이다.

이때부터 **Microsoft**가 **OpenAI**에 본격적으로 눈길을 돌렸다. 이미 **2019년**에 **Azure 클라우드**를 통해 일부 협력 관계를 맺고 있었지만, 이러한 **창발적 능력**으로 인한 **LLM의 능력 극대화**는 충격적으로 다가왔을 것이다.

**OpenAI**의 CEO인 **샘 알트먼**과 연구진들은 이러한 **창발적 능력**을 **2017년**에 알게 되어 **GPT-1부터 3까지 Zero-shot, Few-shot** 등 여러 가지 현상을 관찰하고 개선하면서 지금에 이르렀다. 참고로 **Zero-shot**과 **Few-shot**은 각각 **GPT-2**와 **GPT-3**에서 처음 체계적으로 입증되었다고 한다.

---

## 📌 마무리
이번 글에서는 **LLM의 역사**와 **기본 원리**, 그리고 **창발적 능력**의 개념까지 정리해 보았다. 다음 글에서는 **LLM의 핵심 구조 Transformer**를 심도 있게 다룰 예정이다. 또한 **LLM이 학습을 하는 방식**도 다룰 생각이다.

[[참고] Unsupervised sentiment neuron](https://openai.com/index/unsupervised-sentiment-neuron/)
